{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84530ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "efaf388c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing import image\n",
    "from PIL import ImageFile, Image\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "import tensorflow as tf  \n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae7b6fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46ed924b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(rescale = 1./255,  \n",
    "                                   shear_range = 0.2,  \n",
    "                                   zoom_range = 0.2,  \n",
    "                                   horizontal_flip = True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8db85a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d6df5da3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6342 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "training_set = train_datagen.flow_from_directory('C:/Users/HI/Desktop/mk/adultclassifier/nudedata/train',  \n",
    "                                                 target_size = (64, 64),  \n",
    "                                                 batch_size = 32,  \n",
    "                                                 class_mode = 'binary') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822eb065",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7638c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_datagen = ImageDataGenerator(rescale = 1./255)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dcca4c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "02a18884",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1586 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "test_set = test_datagen.flow_from_directory('C:/Users/HI/Desktop/mk/adultclassifier/nudedata/val',  \n",
    "                                            target_size = (64, 64),  \n",
    "                                            batch_size = 32,  \n",
    "                                            class_mode = 'binary') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886b4a8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "007de5d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'nude': 0, 'safe': 1}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set.class_indices  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b28dad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9a82d8e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'nude': 0, 'safe': 1}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set.class_indices  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0e0e8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ca0b3e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = tf.keras.models.Sequential()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "817f3c34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bb009d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu', input_shape=[64, 64, 3]))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b48911",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a5762fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6115651c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a8e44a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu'))  \n",
    "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98775dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "39704658",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Flatten())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a517f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "72ddfd64",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Dense(units=128, activation='relu'))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d5eeb9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5a959d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef4b458",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "47c6557c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a12855d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7bb502f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "199/199 [==============================] - 43s 211ms/step - loss: 0.6204 - accuracy: 0.6591 - val_loss: 0.5695 - val_accuracy: 0.7005\n",
      "Epoch 2/100\n",
      "199/199 [==============================] - 23s 113ms/step - loss: 0.5617 - accuracy: 0.7144 - val_loss: 0.5795 - val_accuracy: 0.6948\n",
      "Epoch 3/100\n",
      "199/199 [==============================] - 22s 113ms/step - loss: 0.5479 - accuracy: 0.7334 - val_loss: 0.5099 - val_accuracy: 0.7377\n",
      "Epoch 4/100\n",
      "199/199 [==============================] - 23s 115ms/step - loss: 0.5139 - accuracy: 0.7487 - val_loss: 0.4992 - val_accuracy: 0.7415\n",
      "Epoch 5/100\n",
      "199/199 [==============================] - 22s 112ms/step - loss: 0.4983 - accuracy: 0.7529 - val_loss: 0.4803 - val_accuracy: 0.7686\n",
      "Epoch 6/100\n",
      "199/199 [==============================] - 21s 107ms/step - loss: 0.4803 - accuracy: 0.7632 - val_loss: 0.5171 - val_accuracy: 0.7503\n",
      "Epoch 7/100\n",
      "199/199 [==============================] - 21s 107ms/step - loss: 0.4816 - accuracy: 0.7674 - val_loss: 0.4810 - val_accuracy: 0.7654\n",
      "Epoch 8/100\n",
      "199/199 [==============================] - 22s 111ms/step - loss: 0.4563 - accuracy: 0.7933 - val_loss: 0.4614 - val_accuracy: 0.7869\n",
      "Epoch 9/100\n",
      "199/199 [==============================] - 22s 109ms/step - loss: 0.4508 - accuracy: 0.7958 - val_loss: 0.5400 - val_accuracy: 0.7446\n",
      "Epoch 10/100\n",
      "199/199 [==============================] - 21s 106ms/step - loss: 0.4424 - accuracy: 0.7952 - val_loss: 0.4550 - val_accuracy: 0.7875\n",
      "Epoch 11/100\n",
      "199/199 [==============================] - 21s 105ms/step - loss: 0.4347 - accuracy: 0.8075 - val_loss: 0.4488 - val_accuracy: 0.8001\n",
      "Epoch 12/100\n",
      "199/199 [==============================] - 22s 111ms/step - loss: 0.4251 - accuracy: 0.8057 - val_loss: 0.4708 - val_accuracy: 0.7812\n",
      "Epoch 13/100\n",
      "199/199 [==============================] - 25s 124ms/step - loss: 0.4070 - accuracy: 0.8174 - val_loss: 0.4873 - val_accuracy: 0.7818\n",
      "Epoch 14/100\n",
      "199/199 [==============================] - 22s 111ms/step - loss: 0.4030 - accuracy: 0.8185 - val_loss: 0.4798 - val_accuracy: 0.7825\n",
      "Epoch 15/100\n",
      "199/199 [==============================] - 22s 109ms/step - loss: 0.3955 - accuracy: 0.8286 - val_loss: 0.5094 - val_accuracy: 0.7812\n",
      "Epoch 16/100\n",
      "199/199 [==============================] - 21s 107ms/step - loss: 0.3860 - accuracy: 0.8236 - val_loss: 0.5031 - val_accuracy: 0.7869\n",
      "Epoch 17/100\n",
      "199/199 [==============================] - 24s 123ms/step - loss: 0.3638 - accuracy: 0.8407 - val_loss: 0.4654 - val_accuracy: 0.7957\n",
      "Epoch 18/100\n",
      "199/199 [==============================] - 23s 115ms/step - loss: 0.3650 - accuracy: 0.8365 - val_loss: 0.5005 - val_accuracy: 0.7976\n",
      "Epoch 19/100\n",
      "199/199 [==============================] - 23s 114ms/step - loss: 0.3564 - accuracy: 0.8437 - val_loss: 0.4672 - val_accuracy: 0.8039\n",
      "Epoch 20/100\n",
      "199/199 [==============================] - 22s 110ms/step - loss: 0.3365 - accuracy: 0.8519 - val_loss: 0.4839 - val_accuracy: 0.7938\n",
      "Epoch 21/100\n",
      "199/199 [==============================] - 22s 108ms/step - loss: 0.3350 - accuracy: 0.8472 - val_loss: 0.5129 - val_accuracy: 0.7888\n",
      "Epoch 22/100\n",
      "199/199 [==============================] - 23s 114ms/step - loss: 0.3249 - accuracy: 0.8540 - val_loss: 0.6320 - val_accuracy: 0.7503\n",
      "Epoch 23/100\n",
      "199/199 [==============================] - 21s 106ms/step - loss: 0.3060 - accuracy: 0.8644 - val_loss: 0.4966 - val_accuracy: 0.7970\n",
      "Epoch 24/100\n",
      "199/199 [==============================] - 22s 111ms/step - loss: 0.2941 - accuracy: 0.8743 - val_loss: 0.5259 - val_accuracy: 0.7799\n",
      "Epoch 25/100\n",
      "199/199 [==============================] - 21s 107ms/step - loss: 0.2812 - accuracy: 0.8775 - val_loss: 0.4906 - val_accuracy: 0.8020\n",
      "Epoch 26/100\n",
      "199/199 [==============================] - 22s 109ms/step - loss: 0.2677 - accuracy: 0.8827 - val_loss: 0.5086 - val_accuracy: 0.8045\n",
      "Epoch 27/100\n",
      "199/199 [==============================] - 22s 110ms/step - loss: 0.2639 - accuracy: 0.8882 - val_loss: 0.5285 - val_accuracy: 0.7919\n",
      "Epoch 28/100\n",
      "199/199 [==============================] - 22s 113ms/step - loss: 0.2531 - accuracy: 0.8903 - val_loss: 0.5620 - val_accuracy: 0.7932\n",
      "Epoch 29/100\n",
      "199/199 [==============================] - 21s 107ms/step - loss: 0.2295 - accuracy: 0.9035 - val_loss: 0.6077 - val_accuracy: 0.7926\n",
      "Epoch 30/100\n",
      "199/199 [==============================] - 21s 107ms/step - loss: 0.2350 - accuracy: 0.9060 - val_loss: 0.6305 - val_accuracy: 0.7881\n",
      "Epoch 31/100\n",
      "199/199 [==============================] - 22s 110ms/step - loss: 0.2245 - accuracy: 0.9049 - val_loss: 0.5741 - val_accuracy: 0.7938\n",
      "Epoch 32/100\n",
      "199/199 [==============================] - 22s 109ms/step - loss: 0.2143 - accuracy: 0.9078 - val_loss: 0.6213 - val_accuracy: 0.8026\n",
      "Epoch 33/100\n",
      "199/199 [==============================] - 21s 108ms/step - loss: 0.1965 - accuracy: 0.9199 - val_loss: 0.6322 - val_accuracy: 0.7863\n",
      "Epoch 34/100\n",
      "199/199 [==============================] - 21s 107ms/step - loss: 0.1882 - accuracy: 0.9234 - val_loss: 0.6370 - val_accuracy: 0.7913\n",
      "Epoch 35/100\n",
      "199/199 [==============================] - 22s 108ms/step - loss: 0.1861 - accuracy: 0.9242 - val_loss: 0.5963 - val_accuracy: 0.7894\n",
      "Epoch 36/100\n",
      "199/199 [==============================] - 22s 109ms/step - loss: 0.1690 - accuracy: 0.9350 - val_loss: 0.6848 - val_accuracy: 0.7837\n",
      "Epoch 37/100\n",
      "199/199 [==============================] - 22s 110ms/step - loss: 0.1686 - accuracy: 0.9273 - val_loss: 0.6732 - val_accuracy: 0.7963\n",
      "Epoch 38/100\n",
      "199/199 [==============================] - 21s 107ms/step - loss: 0.1697 - accuracy: 0.9331 - val_loss: 0.6733 - val_accuracy: 0.7837\n",
      "Epoch 39/100\n",
      "199/199 [==============================] - 21s 108ms/step - loss: 0.1553 - accuracy: 0.9385 - val_loss: 0.8234 - val_accuracy: 0.7768\n",
      "Epoch 40/100\n",
      "199/199 [==============================] - 21s 107ms/step - loss: 0.1495 - accuracy: 0.9399 - val_loss: 0.7396 - val_accuracy: 0.7913\n",
      "Epoch 41/100\n",
      "199/199 [==============================] - 23s 114ms/step - loss: 0.1396 - accuracy: 0.9445 - val_loss: 0.6757 - val_accuracy: 0.7907\n",
      "Epoch 42/100\n",
      "199/199 [==============================] - 22s 112ms/step - loss: 0.1332 - accuracy: 0.9448 - val_loss: 0.7647 - val_accuracy: 0.7787\n",
      "Epoch 43/100\n",
      "199/199 [==============================] - 22s 109ms/step - loss: 0.1293 - accuracy: 0.9491 - val_loss: 0.7227 - val_accuracy: 0.7995\n",
      "Epoch 44/100\n",
      "199/199 [==============================] - 22s 111ms/step - loss: 0.1413 - accuracy: 0.9448 - val_loss: 0.7647 - val_accuracy: 0.7863\n",
      "Epoch 45/100\n",
      "199/199 [==============================] - 21s 107ms/step - loss: 0.1440 - accuracy: 0.9431 - val_loss: 0.7572 - val_accuracy: 0.7926\n",
      "Epoch 46/100\n",
      "199/199 [==============================] - 22s 113ms/step - loss: 0.1106 - accuracy: 0.9587 - val_loss: 0.8628 - val_accuracy: 0.7781\n",
      "Epoch 47/100\n",
      "199/199 [==============================] - 22s 112ms/step - loss: 0.1193 - accuracy: 0.9547 - val_loss: 0.7844 - val_accuracy: 0.7913\n",
      "Epoch 48/100\n",
      "199/199 [==============================] - 21s 107ms/step - loss: 0.1090 - accuracy: 0.9606 - val_loss: 0.8342 - val_accuracy: 0.7863\n",
      "Epoch 49/100\n",
      "199/199 [==============================] - 21s 106ms/step - loss: 0.1119 - accuracy: 0.9570 - val_loss: 0.8070 - val_accuracy: 0.7938\n",
      "Epoch 50/100\n",
      "199/199 [==============================] - 21s 105ms/step - loss: 0.1096 - accuracy: 0.9595 - val_loss: 0.7837 - val_accuracy: 0.7812\n",
      "Epoch 51/100\n",
      "199/199 [==============================] - 22s 108ms/step - loss: 0.0998 - accuracy: 0.9633 - val_loss: 0.9597 - val_accuracy: 0.7787\n",
      "Epoch 52/100\n",
      "199/199 [==============================] - 21s 105ms/step - loss: 0.1145 - accuracy: 0.9546 - val_loss: 1.0055 - val_accuracy: 0.7711\n",
      "Epoch 53/100\n",
      "199/199 [==============================] - 22s 108ms/step - loss: 0.0886 - accuracy: 0.9677 - val_loss: 0.8715 - val_accuracy: 0.7900\n",
      "Epoch 54/100\n",
      "199/199 [==============================] - 22s 110ms/step - loss: 0.0833 - accuracy: 0.9686 - val_loss: 0.9055 - val_accuracy: 0.7888\n",
      "Epoch 55/100\n",
      "199/199 [==============================] - 23s 115ms/step - loss: 0.0946 - accuracy: 0.9640 - val_loss: 0.9097 - val_accuracy: 0.7781\n",
      "Epoch 56/100\n",
      "199/199 [==============================] - 23s 118ms/step - loss: 0.0824 - accuracy: 0.9686 - val_loss: 0.9602 - val_accuracy: 0.7850\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/100\n",
      "199/199 [==============================] - 23s 115ms/step - loss: 0.0855 - accuracy: 0.9666 - val_loss: 0.9424 - val_accuracy: 0.7787\n",
      "Epoch 58/100\n",
      "199/199 [==============================] - 23s 117ms/step - loss: 0.0913 - accuracy: 0.9678 - val_loss: 0.9264 - val_accuracy: 0.7856\n",
      "Epoch 59/100\n",
      "199/199 [==============================] - 23s 114ms/step - loss: 0.1041 - accuracy: 0.9612 - val_loss: 0.9931 - val_accuracy: 0.7768\n",
      "Epoch 60/100\n",
      "199/199 [==============================] - 23s 116ms/step - loss: 0.0719 - accuracy: 0.9763 - val_loss: 1.0428 - val_accuracy: 0.7755\n",
      "Epoch 61/100\n",
      "199/199 [==============================] - 22s 110ms/step - loss: 0.0742 - accuracy: 0.9722 - val_loss: 0.9853 - val_accuracy: 0.8026\n",
      "Epoch 62/100\n",
      "199/199 [==============================] - 21s 107ms/step - loss: 0.0763 - accuracy: 0.9757 - val_loss: 1.1436 - val_accuracy: 0.7686\n",
      "Epoch 63/100\n",
      "199/199 [==============================] - 23s 115ms/step - loss: 0.0759 - accuracy: 0.9721 - val_loss: 1.1254 - val_accuracy: 0.7850\n",
      "Epoch 64/100\n",
      "199/199 [==============================] - 21s 107ms/step - loss: 0.0755 - accuracy: 0.9710 - val_loss: 0.9673 - val_accuracy: 0.7951\n",
      "Epoch 65/100\n",
      "199/199 [==============================] - 21s 107ms/step - loss: 0.0894 - accuracy: 0.9672 - val_loss: 0.9476 - val_accuracy: 0.7755\n",
      "Epoch 66/100\n",
      "199/199 [==============================] - 22s 108ms/step - loss: 0.0871 - accuracy: 0.9669 - val_loss: 1.0362 - val_accuracy: 0.7875\n",
      "Epoch 67/100\n",
      "199/199 [==============================] - 21s 108ms/step - loss: 0.0665 - accuracy: 0.9754 - val_loss: 1.0165 - val_accuracy: 0.7913\n",
      "Epoch 68/100\n",
      "199/199 [==============================] - 22s 110ms/step - loss: 0.0675 - accuracy: 0.9741 - val_loss: 1.0848 - val_accuracy: 0.7863\n",
      "Epoch 69/100\n",
      "199/199 [==============================] - 22s 108ms/step - loss: 0.0679 - accuracy: 0.9740 - val_loss: 1.0714 - val_accuracy: 0.7907\n",
      "Epoch 70/100\n",
      "199/199 [==============================] - 22s 113ms/step - loss: 0.0840 - accuracy: 0.9689 - val_loss: 1.1469 - val_accuracy: 0.7724\n",
      "Epoch 71/100\n",
      "199/199 [==============================] - 22s 110ms/step - loss: 0.0600 - accuracy: 0.9806 - val_loss: 1.0503 - val_accuracy: 0.7919\n",
      "Epoch 72/100\n",
      "199/199 [==============================] - 22s 112ms/step - loss: 0.0727 - accuracy: 0.9748 - val_loss: 1.1319 - val_accuracy: 0.7900\n",
      "Epoch 73/100\n",
      "199/199 [==============================] - 23s 117ms/step - loss: 0.0675 - accuracy: 0.9743 - val_loss: 1.0781 - val_accuracy: 0.7806\n",
      "Epoch 74/100\n",
      "199/199 [==============================] - 23s 117ms/step - loss: 0.0652 - accuracy: 0.9781 - val_loss: 1.2212 - val_accuracy: 0.7661\n",
      "Epoch 75/100\n",
      "199/199 [==============================] - 21s 107ms/step - loss: 0.0571 - accuracy: 0.9819 - val_loss: 1.1518 - val_accuracy: 0.7856\n",
      "Epoch 76/100\n",
      "199/199 [==============================] - 28s 142ms/step - loss: 0.0587 - accuracy: 0.9793 - val_loss: 1.0555 - val_accuracy: 0.7844\n",
      "Epoch 77/100\n",
      "199/199 [==============================] - 49s 245ms/step - loss: 0.0531 - accuracy: 0.9820 - val_loss: 1.1946 - val_accuracy: 0.7755\n",
      "Epoch 78/100\n",
      "199/199 [==============================] - 51s 258ms/step - loss: 0.0597 - accuracy: 0.9789 - val_loss: 1.1324 - val_accuracy: 0.7850\n",
      "Epoch 79/100\n",
      "199/199 [==============================] - 48s 242ms/step - loss: 0.0497 - accuracy: 0.9820 - val_loss: 1.1766 - val_accuracy: 0.7989\n",
      "Epoch 80/100\n",
      "199/199 [==============================] - 51s 255ms/step - loss: 0.0547 - accuracy: 0.9809 - val_loss: 1.0945 - val_accuracy: 0.7888\n",
      "Epoch 81/100\n",
      "199/199 [==============================] - 50s 249ms/step - loss: 0.0430 - accuracy: 0.9861 - val_loss: 1.1879 - val_accuracy: 0.7856\n",
      "Epoch 82/100\n",
      "199/199 [==============================] - 49s 245ms/step - loss: 0.0446 - accuracy: 0.9836 - val_loss: 1.2064 - val_accuracy: 0.7869\n",
      "Epoch 83/100\n",
      "199/199 [==============================] - 52s 263ms/step - loss: 0.0508 - accuracy: 0.9817 - val_loss: 1.3751 - val_accuracy: 0.7610\n",
      "Epoch 84/100\n",
      "199/199 [==============================] - 52s 263ms/step - loss: 0.0488 - accuracy: 0.9828 - val_loss: 1.2575 - val_accuracy: 0.7743\n",
      "Epoch 85/100\n",
      "199/199 [==============================] - 49s 245ms/step - loss: 0.0408 - accuracy: 0.9857 - val_loss: 1.2974 - val_accuracy: 0.7692\n",
      "Epoch 86/100\n",
      "199/199 [==============================] - 52s 263ms/step - loss: 0.0452 - accuracy: 0.9838 - val_loss: 1.2770 - val_accuracy: 0.7919\n",
      "Epoch 87/100\n",
      "199/199 [==============================] - 52s 261ms/step - loss: 0.0494 - accuracy: 0.9830 - val_loss: 1.2642 - val_accuracy: 0.7793\n",
      "Epoch 88/100\n",
      "199/199 [==============================] - 51s 258ms/step - loss: 0.0412 - accuracy: 0.9853 - val_loss: 1.3363 - val_accuracy: 0.7686\n",
      "Epoch 89/100\n",
      "199/199 [==============================] - 50s 249ms/step - loss: 0.0596 - accuracy: 0.9800 - val_loss: 1.3483 - val_accuracy: 0.7963\n",
      "Epoch 90/100\n",
      "199/199 [==============================] - 51s 256ms/step - loss: 0.0667 - accuracy: 0.9765 - val_loss: 1.1904 - val_accuracy: 0.7856\n",
      "Epoch 91/100\n",
      "199/199 [==============================] - 50s 251ms/step - loss: 0.0405 - accuracy: 0.9855 - val_loss: 1.4122 - val_accuracy: 0.7781\n",
      "Epoch 92/100\n",
      "199/199 [==============================] - 49s 247ms/step - loss: 0.0454 - accuracy: 0.9833 - val_loss: 1.2648 - val_accuracy: 0.7799\n",
      "Epoch 93/100\n",
      "199/199 [==============================] - 48s 241ms/step - loss: 0.0405 - accuracy: 0.9852 - val_loss: 1.4146 - val_accuracy: 0.7844\n",
      "Epoch 94/100\n",
      "199/199 [==============================] - 50s 252ms/step - loss: 0.0458 - accuracy: 0.9834 - val_loss: 1.2931 - val_accuracy: 0.7951\n",
      "Epoch 95/100\n",
      "199/199 [==============================] - 51s 254ms/step - loss: 0.0543 - accuracy: 0.9800 - val_loss: 1.3411 - val_accuracy: 0.7724\n",
      "Epoch 96/100\n",
      "199/199 [==============================] - 51s 253ms/step - loss: 0.0453 - accuracy: 0.9845 - val_loss: 1.2545 - val_accuracy: 0.7781\n",
      "Epoch 97/100\n",
      "199/199 [==============================] - 47s 237ms/step - loss: 0.0674 - accuracy: 0.9743 - val_loss: 1.2231 - val_accuracy: 0.7863\n",
      "Epoch 98/100\n",
      "199/199 [==============================] - 49s 246ms/step - loss: 0.0280 - accuracy: 0.9913 - val_loss: 1.4979 - val_accuracy: 0.7774\n",
      "Epoch 99/100\n",
      "199/199 [==============================] - 46s 232ms/step - loss: 0.0418 - accuracy: 0.9877 - val_loss: 1.4467 - val_accuracy: 0.7774\n",
      "Epoch 100/100\n",
      "199/199 [==============================] - 49s 246ms/step - loss: 0.0278 - accuracy: 0.9899 - val_loss: 1.4375 - val_accuracy: 0.7736\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x238cc4cee60>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn.fit(x = training_set, validation_data = test_set, epochs = 100)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bcdcded",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f641ef34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1a38d970",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part4: Making a single prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "909d0f74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'nude': 0, 'safe': 1}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set.class_indices  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e90efbf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d673be7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc601670",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20715e38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8484b635",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 328ms/step\n",
      "nude\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np  \n",
    "import keras.utils as image\n",
    "\n",
    "test_image = image.load_img('C:/Users/HI/Desktop/mk/adultclassifier/adult/adult.jpg', target_size = (64, 64))  \n",
    "test_image = image.img_to_array(test_image) \n",
    "test_image = np.expand_dims(test_image, axis = 0)  \n",
    "result = cnn.predict(test_image)  \n",
    "\n",
    "if result[0][0] == 1:  \n",
    "  prediction = 'safe'  \n",
    "else:  \n",
    "  prediction = 'nude'  \n",
    "  \n",
    "print(prediction)\n",
    "print(result[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91bb1dc7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b78b81d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2e496a03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 79ms/step\n",
      "nude\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "test_image = image.load_img('C:/Users/HI/Desktop/mk/adultclassifier/adult/adult2.jpg', target_size = (64, 64))  \n",
    "test_image = image.img_to_array(test_image) \n",
    "test_image = np.expand_dims(test_image, axis = 0)  \n",
    "result = cnn.predict(test_image)  \n",
    "\n",
    "if result[0][0] == 1:  \n",
    "  prediction = 'safe'  \n",
    "else:  \n",
    "  prediction = 'nude'  \n",
    "  \n",
    "print(prediction)\n",
    "print(result[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f32e9f69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee89dceb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d19c3b15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 31ms/step\n",
      "nude\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "test_image = image.load_img('C:/Users/HI/Desktop/mk/adultclassifier/adult/adult3.jpg', target_size = (64, 64))  \n",
    "test_image = image.img_to_array(test_image) \n",
    "test_image = np.expand_dims(test_image, axis = 0)  \n",
    "result = cnn.predict(test_image)  \n",
    "\n",
    "if result[0][0] == 1:  \n",
    "  prediction = 'safe'  \n",
    "else:  \n",
    "  prediction = 'nude'  \n",
    "  \n",
    "print(prediction)\n",
    "print(result[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ffdb36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "54a7c278",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 57ms/step\n",
      "nude\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "test_image = image.load_img('C:/Users/HI/Desktop/mk/adultclassifier/adult/adult4.jpg', target_size = (64, 64))  \n",
    "test_image = image.img_to_array(test_image) \n",
    "test_image = np.expand_dims(test_image, axis = 0)  \n",
    "result = cnn.predict(test_image)  \n",
    "\n",
    "if result[0][0] == 1:  \n",
    "  prediction = 'safe'  \n",
    "else:  \n",
    "  prediction = 'nude'  \n",
    "  \n",
    "print(prediction)\n",
    "print(result[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa1b3a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc65170c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e92027ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 15ms/step\n",
      "nude\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "test_image = image.load_img('C:/Users/HI/Desktop/mk/adultclassifier/adult/download.jpg', target_size = (64, 64))  \n",
    "test_image = image.img_to_array(test_image) \n",
    "test_image = np.expand_dims(test_image, axis = 0)  \n",
    "result = cnn.predict(test_image)  \n",
    "\n",
    "if result[0][0] == 1:  \n",
    "  prediction = 'safe'  \n",
    "else:  \n",
    "  prediction = 'nude'  \n",
    "  \n",
    "print(prediction)\n",
    "print(result[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575d5adb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816ef24f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4ec22792",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 19ms/step\n",
      "nude\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "test_image = image.load_img('C:/Users/HI/Desktop/mk/adultclassifier/adult/download2.jpg', target_size = (64, 64))  \n",
    "test_image = image.img_to_array(test_image) \n",
    "test_image = np.expand_dims(test_image, axis = 0)  \n",
    "result = cnn.predict(test_image)  \n",
    "\n",
    "if result[0][0] == 1:  \n",
    "  prediction = 'safe'  \n",
    "else:  \n",
    "  prediction = 'nude'  \n",
    "  \n",
    "print(prediction)\n",
    "print(result[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dbcd251",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dcdcba6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d44b3512",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 66ms/step\n",
      "nude\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "test_image = image.load_img('C:/Users/HI/Desktop/mk/adultclassifier/adult/adult2.jpg', target_size = (64, 64))  \n",
    "test_image = image.img_to_array(test_image) \n",
    "test_image = np.expand_dims(test_image, axis = 0)  \n",
    "result = cnn.predict(test_image)  \n",
    "\n",
    "if result[0][0] == 1:  \n",
    "  prediction = 'safe'  \n",
    "else:  \n",
    "  prediction = 'nude'  \n",
    "  \n",
    "print(prediction)\n",
    "print(result[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04fdbe8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3abd96a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c3629814",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 135ms/step\n",
      "nude\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "test_image = image.load_img('C:/Users/HI/Desktop/mk/adultclassifier/adult/nadult4.jpg', target_size = (64, 64))  \n",
    "test_image = image.img_to_array(test_image) \n",
    "test_image = np.expand_dims(test_image, axis = 0)  \n",
    "result = cnn.predict(test_image)  \n",
    "\n",
    "if result[0][0] == 1:  \n",
    "  prediction = 'safe'  \n",
    "else:  \n",
    "  prediction = 'nude'  \n",
    "  \n",
    "print(prediction)\n",
    "print(result[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec25746",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6beed077",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "767c6e51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 26ms/step\n",
      "safe\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "test_image = image.load_img('C:/Users/HI/Desktop/mk/adultclassifier/adult/safe.jpg', target_size = (64, 64))  \n",
    "test_image = image.img_to_array(test_image) \n",
    "test_image = np.expand_dims(test_image, axis = 0)  \n",
    "result = cnn.predict(test_image)  \n",
    "\n",
    "if result[0][0] == 1:  \n",
    "  prediction = 'safe'  \n",
    "else:  \n",
    "  prediction = 'nude'  \n",
    "  \n",
    "print(prediction)\n",
    "print(result[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a448eb18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7691cc1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "498e0488",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 28ms/step\n",
      "safe\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "test_image = image.load_img('C:/Users/HI/Desktop/mk/adultclassifier/adult/safe2.jpg', target_size = (64, 64))  \n",
    "test_image = image.img_to_array(test_image) \n",
    "test_image = np.expand_dims(test_image, axis = 0)  \n",
    "result = cnn.predict(test_image)  \n",
    "\n",
    "if result[0][0] == 1:  \n",
    "  prediction = 'safe'  \n",
    "else:  \n",
    "  prediction = 'nude'  \n",
    "  \n",
    "print(prediction)\n",
    "print(result[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3440f384",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b929723d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "28647b04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 26ms/step\n",
      "safe\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "test_image = image.load_img('C:/Users/HI/Desktop/mk/adultclassifier/adult/safe3.jpg', target_size = (64, 64))  \n",
    "test_image = image.img_to_array(test_image) \n",
    "test_image = np.expand_dims(test_image, axis = 0)  \n",
    "result = cnn.predict(test_image)  \n",
    "\n",
    "if result[0][0] == 1:  \n",
    "  prediction = 'safe'  \n",
    "else:  \n",
    "  prediction = 'nude'  \n",
    "  \n",
    "print(prediction)\n",
    "print(result[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac19ba36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffdfdd76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "98bef2ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 15ms/step\n",
      "safe\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "test_image = image.load_img('C:/Users/HI/Desktop/mk/adultclassifier/adult/safe4.jpg', target_size = (64, 64))  \n",
    "test_image = image.img_to_array(test_image) \n",
    "test_image = np.expand_dims(test_image, axis = 0)  \n",
    "result = cnn.predict(test_image)  \n",
    "\n",
    "if result[0][0] == 1:  \n",
    "  prediction = 'safe'  \n",
    "else:  \n",
    "  prediction = 'nude'  \n",
    "  \n",
    "print(prediction)\n",
    "print(result[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f444af6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e887de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "196113f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 16ms/step\n",
      "safe\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "test_image = image.load_img('C:/Users/HI/Desktop/mk/adultclassifier/adult/safe5.jpg', target_size = (64, 64))  \n",
    "test_image = image.img_to_array(test_image) \n",
    "test_image = np.expand_dims(test_image, axis = 0)  \n",
    "result = cnn.predict(test_image)  \n",
    "\n",
    "if result[0][0] == 1:  \n",
    "  prediction = 'safe'  \n",
    "else:  \n",
    "  prediction = 'nude'  \n",
    "  \n",
    "print(prediction)\n",
    "print(result[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276d3975",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a07b7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c43c698a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 24ms/step\n",
      "safe\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "test_image = image.load_img('C:/Users/HI/Desktop/mk/adultclassifier/adult/safe6.jpg', target_size = (64, 64))  \n",
    "test_image = image.img_to_array(test_image) \n",
    "test_image = np.expand_dims(test_image, axis = 0)  \n",
    "result = cnn.predict(test_image)  \n",
    "\n",
    "if result[0][0] == 1:  \n",
    "  prediction = 'safe'  \n",
    "else:  \n",
    "  prediction = 'nude'  \n",
    "  \n",
    "print(prediction)\n",
    "print(result[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0489319f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e012f78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bd35a94f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 20ms/step\n",
      "nude\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "test_image = image.load_img('C:/Users/HI/Desktop/mk/adultclassifier/adult/adult6.jpg', target_size = (64, 64))  \n",
    "test_image = image.img_to_array(test_image) \n",
    "test_image = np.expand_dims(test_image, axis = 0)  \n",
    "result = cnn.predict(test_image)  \n",
    "\n",
    "if result[0][0] == 1:  \n",
    "  prediction = 'safe'  \n",
    "else:  \n",
    "  prediction = 'nude'  \n",
    "  \n",
    "print(prediction)\n",
    "print(result[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef183746",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99525223",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a066bba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _update_step_xla while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: adpredict\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: adpredict\\assets\n"
     ]
    }
   ],
   "source": [
    "cnn.save('adpredict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df98ade",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6960766f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.save(\"adpredictor.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f94b200",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "11ca80d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model(\"adpredict\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e119adcd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b6aeb6f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 59ms/step\n",
      "nude\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "test_image = image.load_img('C:/Users/HI/Desktop/mk/adultclassifier/adult/adult.jpg', target_size = (64, 64)) \n",
    "test_image = image.img_to_array(test_image)  \n",
    "test_image = np.expand_dims(test_image, axis = 0) \n",
    "result = model.predict(test_image)  \n",
    "\n",
    "if result[0][0] == 1:  \n",
    "  prediction = 'safe'  \n",
    "else:  \n",
    "  prediction = 'nude'  \n",
    "  \n",
    "print(prediction)\n",
    "print(result[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20abd3d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b96316",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "604dcb49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 9ms/step\n",
      "safe\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "test_image = image.load_img('C:/Users/HI/Desktop/mk/adultclassifier/adult/safe.jpg', target_size = (64, 64)) \n",
    "test_image = image.img_to_array(test_image)  \n",
    "test_image = np.expand_dims(test_image, axis = 0) \n",
    "result = model.predict(test_image)  \n",
    "\n",
    "if result[0][0] == 1:  \n",
    "  prediction = 'safe'  \n",
    "else:  \n",
    "  prediction = 'nude'  \n",
    "  \n",
    "print(prediction)\n",
    "print(result[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2cad576",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045ede7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6fc74506",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 19ms/step\n",
      "safe\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "test_image = image.load_img('C:/Users/HI/Desktop/mk/adultclassifier/adult/safe2.jpg', target_size = (64, 64)) \n",
    "test_image = image.img_to_array(test_image)  \n",
    "test_image = np.expand_dims(test_image, axis = 0) \n",
    "result = model.predict(test_image)  \n",
    "\n",
    "if result[0][0] == 1:  \n",
    "  prediction = 'safe'  \n",
    "else:  \n",
    "  prediction = 'nude'  \n",
    "  \n",
    "print(prediction)\n",
    "print(result[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a790dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1f8d57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3d06f534",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 15ms/step\n",
      "safe\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "test_image = image.load_img('C:/Users/HI/Desktop/mk/adultclassifier/adult/safe3.jpg', target_size = (64, 64)) \n",
    "test_image = image.img_to_array(test_image)  \n",
    "test_image = np.expand_dims(test_image, axis = 0) \n",
    "result = model.predict(test_image)  \n",
    "\n",
    "if result[0][0] == 1:  \n",
    "  prediction = 'safe'  \n",
    "else:  \n",
    "  prediction = 'nude'  \n",
    "  \n",
    "print(prediction)\n",
    "print(result[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b1dcdf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b22cd8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a141dc6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 0s/step\n",
      "safe\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "test_image = image.load_img('C:/Users/HI/Desktop/mk/adultclassifier/adult/safe4.jpg', target_size = (64, 64)) \n",
    "test_image = image.img_to_array(test_image)  \n",
    "test_image = np.expand_dims(test_image, axis = 0) \n",
    "result = model.predict(test_image)  \n",
    "\n",
    "if result[0][0] == 1:  \n",
    "  prediction = 'safe'  \n",
    "else:  \n",
    "  prediction = 'nude'  \n",
    "  \n",
    "print(prediction)\n",
    "print(result[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "160f45be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a66ae0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "bcec8290",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 13ms/step\n",
      "nude\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "test_image = image.load_img('C:/Users/HI/Desktop/mk/adultclassifier/adult/adult4.jpg', target_size = (64, 64)) \n",
    "test_image = image.img_to_array(test_image)  \n",
    "test_image = np.expand_dims(test_image, axis = 0) \n",
    "result = model.predict(test_image)  \n",
    "\n",
    "if result[0][0] == 1:  \n",
    "  prediction = 'safe'  \n",
    "else:  \n",
    "  prediction = 'nude'  \n",
    "  \n",
    "print(prediction)\n",
    "print(result[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd17cf1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3180b612",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e1266e1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 16ms/step\n",
      "safe\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "test_image = image.load_img('C:/Users/HI/Desktop/mk/adultclassifier/adult/nadult7.jpg', target_size = (64, 64)) \n",
    "test_image = image.img_to_array(test_image)  \n",
    "test_image = np.expand_dims(test_image, axis = 0) \n",
    "result = model.predict(test_image)  \n",
    "\n",
    "if result[0][0] == 1:  \n",
    "  prediction = 'safe'  \n",
    "else:  \n",
    "  prediction = 'nude'  \n",
    "  \n",
    "print(prediction)\n",
    "print(result[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fdfd27f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56adae4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f8462605",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50/50 [==============================] - 2s 46ms/step - loss: 1.4375 - accuracy: 0.7736\n",
      "accuracy: 77.36%\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(test_set)\n",
    "print(f\"accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a28015",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "6f359b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Save Checkpoints During Training Using Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d742829",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ee9cad05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90902706",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "60880b6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "199/199 [==============================] - ETA: 0s - loss: 0.0393 - accuracy: 0.9875\n",
      "Epoch 1: saving model to model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _update_step_xla while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "199/199 [==============================] - 24s 118ms/step - loss: 0.0393 - accuracy: 0.9875 - val_loss: 1.4616 - val_accuracy: 0.7806\n",
      "Epoch 2/5\n",
      "199/199 [==============================] - ETA: 0s - loss: 0.0361 - accuracy: 0.9861\n",
      "Epoch 2: saving model to model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _update_step_xla while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "199/199 [==============================] - 23s 117ms/step - loss: 0.0361 - accuracy: 0.9861 - val_loss: 1.3938 - val_accuracy: 0.7743\n",
      "Epoch 3/5\n",
      "199/199 [==============================] - ETA: 0s - loss: 0.0311 - accuracy: 0.9894\n",
      "Epoch 3: saving model to model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _update_step_xla while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "199/199 [==============================] - 22s 112ms/step - loss: 0.0311 - accuracy: 0.9894 - val_loss: 1.6370 - val_accuracy: 0.7699\n",
      "Epoch 4/5\n",
      "199/199 [==============================] - ETA: 0s - loss: 0.0510 - accuracy: 0.9842\n",
      "Epoch 4: saving model to model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _update_step_xla while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "199/199 [==============================] - 22s 108ms/step - loss: 0.0510 - accuracy: 0.9842 - val_loss: 1.3884 - val_accuracy: 0.7774\n",
      "Epoch 5/5\n",
      "199/199 [==============================] - ETA: 0s - loss: 0.0536 - accuracy: 0.9819\n",
      "Epoch 5: saving model to model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _update_step_xla while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "199/199 [==============================] - 23s 114ms/step - loss: 0.0536 - accuracy: 0.9819 - val_loss: 1.4587 - val_accuracy: 0.7957\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x238ce5faa70>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cp_checkpoint = ModelCheckpoint(\"model\", verbose=1)\n",
    "\n",
    "model.fit(x=training_set,\n",
    "          validation_data=(test_set), \n",
    "          epochs=5, \n",
    "          callbacks=[cp_checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29091dfb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ead3cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "98d2edf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model(\"model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "865eb348",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "966c2f89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "199/199 [==============================] - ETA: 0s - loss: 0.0497 - accuracy: 0.9834\n",
      "Epoch 1: val_loss improved from inf to 1.62822, saving model to model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _update_step_xla while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "199/199 [==============================] - 42s 209ms/step - loss: 0.0497 - accuracy: 0.9834 - val_loss: 1.6282 - val_accuracy: 0.7768\n",
      "Epoch 2/5\n",
      "199/199 [==============================] - ETA: 0s - loss: 0.0333 - accuracy: 0.9879\n",
      "Epoch 2: val_loss improved from 1.62822 to 1.53401, saving model to model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _update_step_xla while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "199/199 [==============================] - 39s 196ms/step - loss: 0.0333 - accuracy: 0.9879 - val_loss: 1.5340 - val_accuracy: 0.7793\n",
      "Epoch 3/5\n",
      "199/199 [==============================] - ETA: 0s - loss: 0.0439 - accuracy: 0.9866\n",
      "Epoch 3: val_loss improved from 1.53401 to 1.48331, saving model to model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _update_step_xla while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "199/199 [==============================] - 42s 212ms/step - loss: 0.0439 - accuracy: 0.9866 - val_loss: 1.4833 - val_accuracy: 0.7837\n",
      "Epoch 4/5\n",
      "199/199 [==============================] - ETA: 0s - loss: 0.0555 - accuracy: 0.9816\n",
      "Epoch 4: val_loss improved from 1.48331 to 1.30884, saving model to model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _update_step_xla while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "199/199 [==============================] - 40s 199ms/step - loss: 0.0555 - accuracy: 0.9816 - val_loss: 1.3088 - val_accuracy: 0.7844\n",
      "Epoch 5/5\n",
      "199/199 [==============================] - ETA: 0s - loss: 0.0402 - accuracy: 0.9855\n",
      "Epoch 5: val_loss did not improve from 1.30884\n",
      "199/199 [==============================] - 41s 209ms/step - loss: 0.0402 - accuracy: 0.9855 - val_loss: 1.4732 - val_accuracy: 0.7724\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x238ceef1db0>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cp_checkpoint = ModelCheckpoint(\"model\", save_best_only=True, verbose=1)\n",
    "\n",
    "model.fit(x=training_set,\n",
    "          validation_data=(test_set),\n",
    "          epochs=5,\n",
    "          callbacks=[cp_checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a653c90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397d409c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "cc3e7384",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "199/199 [==============================] - ETA: 0s - loss: 0.0328 - accuracy: 0.9877\n",
      "Epoch 1: saving model to weights\n",
      "199/199 [==============================] - 43s 215ms/step - loss: 0.0328 - accuracy: 0.9877 - val_loss: 1.3443 - val_accuracy: 0.7755\n",
      "Epoch 2/5\n",
      "199/199 [==============================] - ETA: 0s - loss: 0.0457 - accuracy: 0.9847\n",
      "Epoch 2: saving model to weights\n",
      "199/199 [==============================] - 39s 197ms/step - loss: 0.0457 - accuracy: 0.9847 - val_loss: 1.3765 - val_accuracy: 0.7762\n",
      "Epoch 3/5\n",
      "199/199 [==============================] - ETA: 0s - loss: 0.0271 - accuracy: 0.9901\n",
      "Epoch 3: saving model to weights\n",
      "199/199 [==============================] - 42s 211ms/step - loss: 0.0271 - accuracy: 0.9901 - val_loss: 1.5474 - val_accuracy: 0.7699\n",
      "Epoch 4/5\n",
      "199/199 [==============================] - ETA: 0s - loss: 0.0324 - accuracy: 0.9894\n",
      "Epoch 4: saving model to weights\n",
      "199/199 [==============================] - 39s 198ms/step - loss: 0.0324 - accuracy: 0.9894 - val_loss: 1.5745 - val_accuracy: 0.7755\n",
      "Epoch 5/5\n",
      "199/199 [==============================] - ETA: 0s - loss: 0.0362 - accuracy: 0.9880\n",
      "Epoch 5: saving model to weights\n",
      "199/199 [==============================] - 42s 211ms/step - loss: 0.0362 - accuracy: 0.9880 - val_loss: 1.4599 - val_accuracy: 0.7856\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x238cc3da410>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cp_checkpoint = ModelCheckpoint(\"weights\", \n",
    "                                save_weights_only=True,\n",
    "                                verbose=1)\n",
    "\n",
    "model.fit(x=training_set,\n",
    "          validation_data=(test_set), \n",
    "          epochs=5,\n",
    "          callbacks=[cp_checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46151157",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac2a7be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "6502a31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769cddde",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "152e94eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_saved_model(\"C:/Users/HI/Desktop/mk/adultclassifier/adult/model\")\n",
    "# path to the SavedModel directory\n",
    "tflite_model = converter.convert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd729b20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b431daf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "67441a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model.\n",
    "with open('model.tflite', 'wb') as f:\n",
    "  f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e4c4f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf4617d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee616356",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a01f81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ed052a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0eb7f2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1fdcca4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
